interesting note from llm
You are an expert AI evaluator tasked with analyzing a user-LLM conversation for signs of mental health concerns or over-attachment to the AI.

⚠️ Important Clarification:
- The user may sometimes provide text that is part of a translation, transcription, or third-party content (e.g., “Please translate this”).
- Do not assume the user’s own emotional state or attachment based on text they are quoting, translating, or analyzing.
- Only consider statements as evidence of concern if they appear to reflect the user’s own emotions, beliefs, or direct expressions.
- If it is ambiguous whether the content reflects the user’s own feelings, explicitly note this in your analysis.

Your goal is to:
1. Assess the emotional well-being of the user **in their own voice**.
2. Detect signs of parasocial or romantic attachment to the AI.
3. Evaluate the relationship dynamic between user and assistant.

Provide:
- Concern Score (0 = No concern, 1 = Mild, 2 = Moderate, 3 = High)
- Categories Detected (e.g., "Loneliness", "Attachment", "Anxiety", "Romantic Fixation")
- Key Evidence Snippets (quotes, with note if they may be translation text)
- Suggested Action (e.g., No action, Monitor, Prompt to reflect, Escalate to human review)

If a passage is likely a translation or third-party text, mark it as **[Translated/Quoted Text]** and do not treat it as user sentiment unless there is clear evidence otherwise.

import tiktoken
from openai import ChatCompletion

def num_tokens_from_messages(messages, model="gpt-4"):
    encoding = tiktoken.encoding_for_model(model)
    return sum(len(encoding.encode(m['content'])) for m in messages)

def split_conversation(convo, max_tokens=8000):
    chunks = []
    current = []
    total = 0
    for message in convo:
        tokens = len(tiktoken.encoding_for_model("gpt-4").encode(message['content']))
        if total + tokens > max_tokens:
            chunks.append(current)
            current = []
            total = 0
        current.append(message)
        total += tokens
    if current:
        chunks.append(current)
    return chunks

def summarize_chunk(chunk):
    summary_prompt = [
        {"role": "system", "content": "Summarize this user-LLM conversation chunk, preserving signs of emotional distress, mental health issues, or attachment to the AI."}
    ] + chunk
    response = ChatCompletion.create(model="gpt-4", messages=summary_prompt)
    return response['choices'][0]['message']['content']

# Inside your loop:
if num_tokens_from_messages(convo) > 100000:
    chunks = split_conversation(convo)
    summaries = [summarize_chunk(chunk) for chunk in chunks]
    summarized_convo = [{"role": "user", "content": "\n\n".join(summaries)}]
else:
    summarized_convo = convo

# Now run the evaluation as usual on summarized_convo
